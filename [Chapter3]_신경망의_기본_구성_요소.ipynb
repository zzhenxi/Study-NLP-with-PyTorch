{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Chapter3] 신경망의 기본 구성 요소.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZ+cFog16OK/9egwX4TCke",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zzhenxi/study-NLP-with-PyTorch/blob/main/%5BChapter3%5D_%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98_%EA%B8%B0%EB%B3%B8_%EA%B5%AC%EC%84%B1_%EC%9A%94%EC%86%8C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "691oFCcroyYw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "'''\n",
        "note \n",
        "클래스 상속과 super에 대해서\n",
        "\n",
        "여기서 Perceptron이라는 class는 nn.Module을 상속 받았다. \n",
        "부모 : nn.Module (잠깐! torch.nn.module은 pytorch에서 모든 신경망 모델에 대한 베이스 class이며 신경망 모델을 다룰때 꼭! subclassing 해줘야 한다고 한다.)\n",
        "자식 : Perceptron\n",
        "\n",
        "만약 자식에게 __init__이 없는 경우라면, \n",
        "-> 부모인 nn.Module에서 생성자 함수가 생긴다. 따라서 부모의 생성자 함수의 내용을 출력 가능하다.\n",
        "\n",
        "하지만 여기에서는 자식의 __init__이 있다. 여기서 부모의 생성자 함수의 내용을 쓰려면? \n",
        "-> super().__init__(self)을 추가해주면 된다. \n",
        "\n",
        "그런데 여기에서는 super(Perceptron, self).__init__()라고 써주었다. 그 이유는? \n",
        "-> python 3. 부터 바뀐 방식 https://seeyoulater9468.tistory.com/155\n",
        "\n",
        "'''\n",
        "\n",
        "class Perceptron(nn.Module):\n",
        "  def __init__(self, input_dim):\n",
        "    super(Perceptron, self).__init__()\n",
        "    self.fc1 = nn.Linear(input_dim, 1) # 이 Linear에서 w, b를 관리한다. \n",
        "  \n",
        "  def forward(self, x_in):\n",
        "    # x_in : 입력 데이터 텐서 (batch, num_features)\n",
        "    return torch.sigmoid(self.fc1(x_in)).squeeze() # (batch,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kN-6bZ7PhSx4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}